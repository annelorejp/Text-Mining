{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here authorship attribution is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections as coll\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "cmuDictionary = None\n",
    "\n",
    "df = pd.read_csv('winemag-data-130k-v2.csv')\n",
    "\n",
    "value_counts = df['taster_name'].value_counts()\n",
    "to_remove = value_counts[value_counts >= 10000].index\n",
    "new_dataset = df[~df.taster_name.isin(to_remove)]\n",
    "voss_only = df[df.taster_name ==\"Roger Voss\"]\n",
    "schachner_only = df[df.taster_name ==\"Michael Schachner\"]\n",
    "keefe_only = df[df.taster_name ==\"Kerin Oâ€™Keefe\"]\n",
    "\n",
    "voss_new = voss_only.drop(np.random.choice(voss_only.index, 15514, replace=False))\n",
    "schachner_new = schachner_only.drop(np.random.choice(schachner_only.index, 5134, replace=False))\n",
    "keefe_new = keefe_only.drop(np.random.choice(keefe_only.index, 776, replace=False))\n",
    "\n",
    "new_dataset = pd.concat([new_dataset, keefe_new], axis=0)\n",
    "new_dataset = pd.concat([new_dataset, schachner_new], axis=0)\n",
    "new_dataset = pd.concat([new_dataset, voss_new], axis=0)\n",
    "\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Christina Pickard']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Fiona Adams']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Carrie Dykes']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Alexander Peartree']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Jeff Jenssen']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Mike DeSimone']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Susan Kostrzewa']\n",
    "new_dataset = new_dataset[new_dataset.taster_name != 'Lauren Buzzeo']\n",
    "new_dataset = new_dataset[new_dataset.taster_twitter_handle != '@AnneInVino']\n",
    "\n",
    "df_new = new_dataset\n",
    "print(df_new['taster_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new[['description', 'taster_name']]\n",
    "\n",
    "df_new = df_new.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    " \n",
    "cnt = Counter()\n",
    "for word in df_new.taster_name:\n",
    "    cnt[word] += 1\n",
    "cnt\n",
    "\n",
    "Counter(df_new.taster_name).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values in the authors column\n",
    "null_values = df_new['taster_name'].isnull().sum()\n",
    "null_values\n",
    "\n",
    "#check null values for description\n",
    "null_values_desc = df_new['description'].isnull().sum()\n",
    "null_values_desc\n",
    "\n",
    "#remove authors that are not known\n",
    "df_new = df_new[df_new['taster_name'].notna()]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df_new.groupby([df_new.taster_name]).count().plot(kind='bar')\n",
    "plot.set_xlabel(\"cuisines\")\n",
    "plot.set_ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_new['description']\n",
    "authors = df_new['taster_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, authors, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(X_test)\n",
    "print(list(predictions[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10])\n",
    "print(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "accuracy= []\n",
    "writers = [ 'Anna Lee C. Iijima', 'Jim Gordon', 'Joe Czerwinski', \"Kerin O'Keefe\", 'Matt Kettman', 'Michael Schachner', 'Paul Gregutt', 'Roger Voss', 'Sean P. Sullivan', 'Virginie Boone']\n",
    "\n",
    "confusion=pd.DataFrame(confusion_matrix(y_test, predictions),columns=writers,index=writers)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion,annot=True,fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    global cmuDictionary\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    return syl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLengthByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountPunctuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLengthByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_nn(text): \n",
    "    sentences_nltk = sent_tokenize(text)\n",
    "    for sentence_nltk in sentences_nltk:\n",
    "        tokens_per_sentence = []\n",
    "        sent_tokens = word_tokenize(sentence_nltk)\n",
    "        tokens_per_sentence.append(sent_tokens)\n",
    "        for token in tokens_per_sentence:\n",
    "            pos_tags_per_sentence = []\n",
    "            pos_tags_per_sentence.append(nltk.pos_tag(token))\n",
    "            word_list = []\n",
    "            pos_list = []\n",
    "            for element in pos_tags_per_sentence:\n",
    "                for otherelement in element:\n",
    "                    word_list.append(otherelement[0])\n",
    "                    pos_list.append(otherelement[1])\n",
    "            number = pos_list.count('NN')\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import vader\n",
    "import spacy\n",
    "#nlp = spacy.load('en') # en_core_web_sm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.options.display.max_rows = 20\n",
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextB(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    score = testimonial.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaD(text):\n",
    "    vader_model = SentimentIntensityAnalyzer() \n",
    "    score = vader_model.polarity_scores(text)\n",
    "    scoore = score['compound']\n",
    "    return scoore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLBY = []\n",
    "CSC = []\n",
    "CP = []\n",
    "ASpW = []\n",
    "YCK = []\n",
    "ASLBC = []\n",
    "SI = []\n",
    "VD = []\n",
    "TB = []\n",
    "NN = []\n",
    "\n",
    "for text in texts:\n",
    "    SLBY.append(Avg_SentLengthByWord(text))\n",
    "    CSC.append(CountSpecialCharacter(text))\n",
    "    CP.append(CountPunctuation(text))\n",
    "    ASpW.append(Avg_Syllable_per_Word(text))\n",
    "    YCK.append(YulesCharacteristicK(text))\n",
    "    ASLBC.append(Avg_SentLengthByCh(text))\n",
    "    SI.append(SimpsonsIndex(text))\n",
    "    VD.append(VaD(text))\n",
    "    TB.append(TextB(text))\n",
    "    NN.append(number_nn(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Average word length'] = SLBY\n",
    "df_new['Special characters'] = CSC\n",
    "df_new['Punctuation'] = CP\n",
    "df_new['Average syllable per word'] = ASpW\n",
    "df_new['Yules'] = YCK\n",
    "df_new['Average characters in a sentence'] = ASLBC\n",
    "df_new[\"Simpson's Index\"] = SI\n",
    "df_new['VADER'] = VD\n",
    "df_new['Textblob'] = TB\n",
    "df_new['Nouns'] = NN\n",
    "authors = df_new['taster_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=['description'], axis=1)\n",
    "df_new = df_new.drop(columns=['taster_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_new\n",
    "features\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=50)\n",
    "RT = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "RT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = RT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf.score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "writers = [ 'Anna Lee C. Iijima', 'Jim Gordon', 'Joe Czerwinski', \"Kerin O'Keefe\", 'Matt Kettman', 'Michael Schachner', 'Paul Gregutt', 'Roger Voss', 'Sean P. Sullivan', 'Virginie Boone']\n",
    "\n",
    "confusion=pd.DataFrame(confusion_matrix(y_test, predictions),columns=writers,index=writers)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(confusion,annot=True,fmt='d')\n",
    "plt.title(\"Confusion matrix with all features on Random \")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.savefig('confusion.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "RT = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "RT.fit(X_train, y_train)\n",
    "predictions = RT.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_DT = DecisionTreeClassifier(random_state=0, max_depth=50)\n",
    "\n",
    "clf_DT.fit(X_train, y_train)\n",
    "\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_DT_10 = DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "\n",
    "clf_DT_10.fit(X_train, y_train)\n",
    "\n",
    "y_pred_DT_10 = clf_DT_10.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_DT_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_DT_5 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "\n",
    "clf_DT_5.fit(X_train, y_train)\n",
    "\n",
    "y_pred_DT_5 = clf_DT_5.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_DT_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_DT_25 = DecisionTreeClassifier(random_state=0, max_depth=25)\n",
    "\n",
    "clf_DT_25.fit(X_train, y_train)\n",
    "\n",
    "y_pred_DT_25 = clf_DT_25.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_DT_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_NB = BernoulliNB().fit(X_train, y_train)\n",
    "y_pred_NB = clf_NB.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here Sentiment Analysis is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import vader\n",
    "import spacy\n",
    "nlp = spacy.load('en') # en_core_web_sm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.options.display.max_rows = 20\n",
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "\n",
    "from textblob import TextBlob\n",
    "df = pd.read_json(\"wine.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound = []\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "for description in df.description: \n",
    "    scores = vader_model.polarity_scores(description)\n",
    "    compound.append(scores['compound'])\n",
    "points_vader = df.points\n",
    "compound_vader = compound\n",
    "\n",
    "df_compound = pd.DataFrame(compound_vader, columns = ['compound'])\n",
    "df_points = pd.DataFrame(points_vader, columns = ['points'])\n",
    "from sklearn.linear_model import LinearRegression\n",
    "horizontal_stack = pd.concat([df_compound, df_points], axis = 1)\n",
    "\n",
    "X = horizontal_stack.iloc[:, 0].values.reshape(-1, 1)\n",
    "Y = horizontal_stack.values[:, 1]\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "Y_pred = lr.predict(X)\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X, Y_pred, color='red')\n",
    "plt.title('Correlation between compound and points')\n",
    "plt.ylabel('points')\n",
    "plt.xlabel('compound')\n",
    "plt.show()\n",
    "display(horizontal_stack.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "data_df = pd.DataFrame({\"compound\": compound_vader, \"point\": points_vader})\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit our data on the scaler object\n",
    "scaled_df = scaler.fit_transform(data_df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=[data_df.columns])\n",
    "ax = scaled_df.plot(alpha=15)\n",
    "ax.legend([\"compound\", \"point\"]);\n",
    "plt.ylim(-6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound = []\n",
    "for description in df.description:\n",
    "    testimonial = TextBlob(description)\n",
    "    scores = testimonial.sentiment.polarity\n",
    "    compound.append(scores)\n",
    "\n",
    "points_blob = df.points\n",
    "compound_blob = compound\n",
    "\n",
    "df_compound = pd.DataFrame(compound_blob, columns = ['compound'])\n",
    "df_points = pd.DataFrame(points_blob, columns = ['points'])\n",
    "from sklearn.linear_model import LinearRegression\n",
    "horizontal_stack = pd.concat([df_compound, df_points], axis = 1)\n",
    "horizontal_stack\n",
    "\n",
    "X1 = horizontal_stack.iloc[:, 0].values.reshape(-1, 1)\n",
    "Y1 = horizontal_stack.values[:, 1]\n",
    "lr = LinearRegression()\n",
    "lr.fit(X1, Y1)\n",
    "Y_pred1 = lr.predict(X1)\n",
    "plt.scatter(X1, Y1)\n",
    "plt.plot(X1, Y_pred1, color='red')\n",
    "plt.title('Correlation between compound and points')\n",
    "plt.ylabel('points')\n",
    "plt.xlabel('compound')\n",
    "plt.show()\n",
    "display(horizontal_stack.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "data_df = pd.DataFrame({\"compound\": compound_blob, \"point\": points_blob})\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(data_df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=[data_df.columns])\n",
    "ax = scaled_df.plot(alpha=15)\n",
    "ax.legend([\"compound\", \"point\"]);\n",
    "plt.ylim(-6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taster with the highest mean points \n",
    "taster_points = pd.DataFrame({\"points\": df.points, \"taster\": df.taster_name})\n",
    "for taster in taster_points: \n",
    "    tasters = taster_points.groupby(['taster'])['points'].mean()\n",
    "tasters.nlargest(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taster with highest mean compound text blob \n",
    "taster_compound_blob = pd.DataFrame({\"compound\": compound_blob, \"taster\": df.taster_name})\n",
    "for taster in taster_compound_blob: \n",
    "    tasters_blob = taster_compound_blob.groupby(['taster'])['compound'].mean()\n",
    "tasters_blob.nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taster with highest mean compound vader\n",
    "taster_compound_vader = pd.DataFrame({\"compound\": compound_vader, \"taster\": df.taster_name})\n",
    "for taster in taster_compound_vader: \n",
    "    tasters_vader = taster_compound_vader.groupby(['taster'])['compound'].mean()\n",
    "tasters_vader.nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df.groupby([df.taster_name]).count()[\"description\"].plot(kind=\"bar\")\n",
    "plot.set_xlabel(\"taster name\")\n",
    "plot.set_ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
